{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, os,typing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name,data):\n",
    "    with open(f'data/{file_name}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = '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'\n",
    "headers = {'Accept': 'application/vnd.github+json',\n",
    "           'Authorization': 'Bearer {0}'.format(api_token)}\n",
    "def mine_repository(repo_url:str):\n",
    "    file_name = repo_url.replace('/','_')\n",
    "    if os.path.exists(f'data/{file_name}.json'):\n",
    "        print(f'Repository {repo_url} already expolred!') \n",
    "        return\n",
    "    print(f'Exploring repository {repo_url}:') \n",
    "    github_log_api_endpoint = f'https://api.github.com/repos/{repo_url}/commits'\n",
    "    r = requests.get(github_log_api_endpoint,headers=headers)\n",
    "    r_git_log = json.loads(r.content)\n",
    "    r.close()\n",
    "    if not isinstance(r_git_log, dict):\n",
    "        page_count = 1\n",
    "        links_str = r.headers['Link']\n",
    "        while links_str:\n",
    "            page_count += 1\n",
    "            next_link = [l.split(';')[0] for l in links_str.split(',') if '; rel=\"next\"' in l]\n",
    "            if next_link:\n",
    "                print(f'\\r Calling next page request = {next_link[0][1:-1]}',end='')\n",
    "                r = requests.get(next_link[0][1:-1],headers=headers)\n",
    "                r_git_log_next = json.loads(r.content)\n",
    "                if not isinstance(r_git_log_next, dict):\n",
    "                    r_git_log.extend(r_git_log_next)\n",
    "                    links_str = r.headers['Link']\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        print('\\r',end='')\n",
    "        print(f'\\t{len(r_git_log)} commits found in repository {repo_url} in {page_count} pages. {\" \"*30}')\n",
    "        for i,commit in enumerate(r_git_log):\n",
    "            print(f'\\r\\tDeep loading commit {i+1} of {len(r_git_log)}',end='')\n",
    "            commit_sha = commit[\"sha\"]\n",
    "            github_commit_api_endpoint = f'{github_log_api_endpoint}/{commit_sha}'\n",
    "            r = requests.get(github_commit_api_endpoint,headers=headers)\n",
    "            r_git_commit = json.loads(r.content)\n",
    "            r.close()\n",
    "            commit.update(r_git_commit) \n",
    "        print(f'\\r\\t{len(r_git_log)} commits deep loaded for repository {repo_url}.') \n",
    "        write_to_file(file_name,r_git_log)\n",
    "    else:\n",
    "        print(f'Could not access repository {repo_url}')\n",
    "        print(f'Error: {r_git_log}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository eBay/parallec already expolred!\n",
      "Repository zixpo/candybar already expolred!\n",
      "Repository mtsar/mtsar already expolred!\n",
      "Repository GrammarViz2/grammarviz2_src already expolred!\n"
     ]
    }
   ],
   "source": [
    "target_repos_list = ['eBay/parallec','zixpo/candybar','mtsar/mtsar','GrammarViz2/grammarviz2_src']\n",
    "\n",
    "for repo_url in target_repos_list:\n",
    "    mine_repository(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952 commits loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "repo_files = os.listdir('data/')\n",
    "\n",
    "all_commits:list[dict] = []\n",
    "for repo_file in repo_files:\n",
    "    with open('data/' + repo_file, 'r', encoding='utf-8') as f:\n",
    "        all_commits.extend(json.load(f)) \n",
    "\n",
    "print(f'{len(all_commits)} commits loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha': <class 'str'>, 'node_id': <class 'str'>, 'commit': <class 'dict'>, 'url': <class 'str'>, 'html_url': <class 'str'>, 'comments_url': <class 'str'>, 'author': <class 'dict'>, 'committer': <class 'dict'>, 'parents': <class 'list'>, 'stats': <class 'dict'>, 'files': <class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in all_commits[1000].keys():\n",
    "    schema[k] = type(all_commits[1000][k])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GitCommitBasic = typing.TypedDict('GitCommitBasic',\n",
    "                                  {'author': dict,\n",
    "                                   'comment_count': int,\n",
    "                                   'committer': dict,\n",
    "                                   'message': str,\n",
    "                                   'tree': dict,\n",
    "                                   'url': str,\n",
    "                                   'verification': dict})\n",
    "GitCommit = typing.TypedDict('GitCommit',\n",
    "                             {'sha': str,\n",
    "                              'node_id': str,\n",
    "                              'commit': GitCommitBasic,\n",
    "                              'url': str,\n",
    "                              'html_url': str,\n",
    "                              'comments_url': str,\n",
    "                              'author': dict,\n",
    "                              'committer': dict,\n",
    "                              'parents': list,\n",
    "                              'stats': dict,\n",
    "                              'files': list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'email', 'name']\n",
      "['avatar_url', 'events_url', 'followers_url', 'following_url', 'gists_url', 'gravatar_id', 'html_url', 'id', 'login', 'node_id', 'organizations_url', 'received_events_url', 'repos_url', 'site_admin', 'starred_url', 'subscriptions_url', 'type', 'url']\n",
      "['date', 'email', 'name']\n",
      "['avatar_url', 'events_url', 'followers_url', 'following_url', 'gists_url', 'gravatar_id', 'html_url', 'id', 'login', 'node_id', 'organizations_url', 'received_events_url', 'repos_url', 'site_admin', 'starred_url', 'subscriptions_url', 'type', 'url']\n",
      "https://api.github.com/repos/zixpo/candybar/git/commits/479b8d693d319c6601c02eb1363ad06e3bee808f\n",
      "https://api.github.com/repos/zixpo/candybar/commits/479b8d693d319c6601c02eb1363ad06e3bee808f\n"
     ]
    }
   ],
   "source": [
    "print(sorted(all_commits[1000]['commit']['author'].keys()))\n",
    "print(sorted(all_commits[1000]['author'].keys()))\n",
    "\n",
    "print(sorted(all_commits[1000]['commit']['committer'].keys()))\n",
    "print(sorted(all_commits[1000]['committer'].keys()))\n",
    "\n",
    "print(all_commits[1000]['commit']['url'])\n",
    "print(all_commits[1000]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t'author': dict,\n",
      "\t'comment_count': int,\n",
      "\t'committer': dict,\n",
      "\t'message': str,\n",
      "\t'tree': dict,\n",
      "\t'url': str,\n",
      "\t'verification': dict,\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in sorted(all_commits[1000]['commit'].keys()):\n",
    "    schema[k] = (type(all_commits[1000]['commit'][k]).__qualname__)\n",
    "for k in schema:\n",
    "    print(f\"\\t'{k}': {schema[k]},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_defec_words =  [ 'bug', 'fix', 'fixed', 'defect', 'issue']\n",
    "\n",
    "list_merge_words =  [ 'Merge','merge','merged']\n",
    "\n",
    "list_format= ['formated','format','formatting']\n",
    "\n",
    "list_comment= ['commented','comment','commenting','comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(val_list:list[int]):\n",
    "    s = sum(val_list)\n",
    "    if s == 0:\n",
    "            return 0;\n",
    "    acc = 0\n",
    "    for val in val_list:       \n",
    "        acc += (val/s) ** 2\n",
    "    return 1- acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_changes(allFiles):\n",
    "    this_l = []\n",
    "    for file in allFiles:\n",
    "        this_l.append(file['changes'])\n",
    "    return calc_entropy(this_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def find_number_mod_directory(allFiles):\n",
    "    list_dir = []\n",
    "    for file in allFiles:\n",
    "            this_file = file['filename']\n",
    "            if \"/\" in this_file:\n",
    "                path = pathlib.PurePath(this_file)\n",
    "                list_dir.append(path.parent.name)\n",
    "    return len(set(list_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' \n",
    "files = []\n",
    "for cm in all_commits:\n",
    "    files.append(cm['files'])\n",
    "file_name = []\n",
    "for file in files:\n",
    "    for this_file in file:\n",
    "        file_name.append(this_file['filename'])\n",
    "   \n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #this_file = file['filename']\n",
    "            file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_ext)\n",
    "    return (set(file_exts))\n",
    "'''\n",
    "src_file_ext = ['R', 'jks', 'Procfile', 'classpath', 'properties', 'gradle', 'yml', 'spec', 'avi', 'png', 'ai', 'ser', 'json', 'sh', 'gpx', 'ttf', 'kwgt', 'csv', 'style', 'dtd', 'xml', 'Rproj', 'pro', 'conf', 'Dockerfile', 'pdf', 'gz', 'jpg', 'class', 'travis', 'html', 'bat', 'gradlew', 'mustache', 'java', 'klck', 'ico', 'pl', 'userdata/keepthisfile', 'jar', 'css', 'prefs', 'project', 'bib']\n",
    "\n",
    "meta_files_ext = ['Procfile', 'classpath', 'properties', 'gradle', 'csv', 'gradlew', 'prefs', 'project','.gitignore']\n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 0 if any(word in file_ext_cm for word in src_file_ext) else 1\n",
    "\n",
    "def get_file_meta(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 1 if any(word in file_ext_cm for word in meta_files_ext) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_to_df(commit_dict:GitCommit):\n",
    "\n",
    "    new_dict = {}\n",
    "\n",
    "    new_dict['DIFF_NS'] =0\n",
    "    new_dict['DIFF_ND'] = find_number_mod_directory(commit_dict['files'])\n",
    "    new_dict['DIFF_NF'] = int(len(commit_dict['files']))\n",
    "    new_dict['DIFF_EN'] = get_entropy_changes(commit_dict['files'])\n",
    " \n",
    "    new_dict['SIZE_LA'] = int(commit_dict['stats']['additions'])\n",
    "    new_dict['SIZE_LD'] = int(commit_dict['stats']['deletions'])\n",
    "    new_dict['SIZE_LT'] = 0\n",
    "    new_dict['SIZE_TFC'] = 0\n",
    "\n",
    "    new_dict['PURP_FIX'] =  1 if any(word.lower() in commit_dict['commit']['message'] for word in list_defec_words) else 0\n",
    "    new_dict['PURP_MR'] = 1 if any(word.lower() in commit_dict['commit']['message'] for word in list_merge_words) else 0\n",
    "    new_dict['PURP_CFT'] =0\n",
    "\n",
    "    new_dict['HIST_NDEV'] =0\n",
    "    new_dict['HIST_AGE'] =0\n",
    "    new_dict['HIST_NUC'] =0\n",
    "\n",
    "    new_dict['EXP_EXP'] =0\n",
    "    new_dict['EXP_REXP'] =0\n",
    "    new_dict['EXP_SEXP'] =0\n",
    "\n",
    "    new_dict['TEXT_CM'] =0\n",
    "\n",
    "    new_dict['SKIP_DOC'] =get_file_types_value(commit_dict['files'])\n",
    "    new_dict['SKIP_MET'] =get_file_meta(commit_dict['files'])\n",
    "    new_dict['SKIP_COM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_comment) else 0\n",
    "    new_dict['SKIP_FRM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_format) else 0\n",
    "    new_dict['SKIP_BLD'] =0\n",
    "\n",
    "    new_dict['LABEL'] = int(('[ci skip]' in commit_dict['commit']['message'].lower()) or ('[skip ci]' in commit_dict['commit']['message'].lower()))\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DIFF_NS  DIFF_ND  DIFF_NF   DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  \\\n",
      "0           0        1        1  0.000000        7        0        0   \n",
      "1           0        0        1  0.000000        3        0        0   \n",
      "2           0        0        1  0.000000        5        0        0   \n",
      "3           0        2        2  0.345679       24       12        0   \n",
      "4           0        0        1  0.000000        1        1        0   \n",
      "...       ...      ...      ...       ...      ...      ...      ...   \n",
      "1947        0        8       16  0.778574      266       54        0   \n",
      "1948        0       34      158  0.974474      735      724        0   \n",
      "1949        0        7        9  0.838786       22       19        0   \n",
      "1950        0        3        3  0.431602       74       75        0   \n",
      "1951        0       57      300  0.977624    49086        0        0   \n",
      "\n",
      "      SIZE_TFC  PURP_FIX  PURP_MR  ...  EXP_EXP  EXP_REXP  EXP_SEXP  TEXT_CM  \\\n",
      "0            0         0        0  ...        0         0         0        0   \n",
      "1            0         0        0  ...        0         0         0        0   \n",
      "2            0         0        0  ...        0         0         0        0   \n",
      "3            0         1        0  ...        0         0         0        0   \n",
      "4            0         0        0  ...        0         0         0        0   \n",
      "...        ...       ...      ...  ...      ...       ...       ...      ...   \n",
      "1947         0         0        0  ...        0         0         0        0   \n",
      "1948         0         0        0  ...        0         0         0        0   \n",
      "1949         0         0        0  ...        0         0         0        0   \n",
      "1950         0         0        0  ...        0         0         0        0   \n",
      "1951         0         0        0  ...        0         0         0        0   \n",
      "\n",
      "      SKIP_DOC  SKIP_MET  SKIP_COM  SKIP_FRM  SKIP_BLD  LABEL  \n",
      "0            0         0         0         0         0      0  \n",
      "1            0         0         0         0         0      0  \n",
      "2            0         0         0         0         0      0  \n",
      "3            0         0         0         0         0      0  \n",
      "4            0         0         0         0         0      0  \n",
      "...        ...       ...       ...       ...       ...    ...  \n",
      "1947         0         0         0         0         0      0  \n",
      "1948         0         0         0         0         0      0  \n",
      "1949         0         0         0         0         0      0  \n",
      "1950         0         0         0         0         0      0  \n",
      "1951         0         0         0         0         0      0  \n",
      "\n",
      "[1952 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "new_format = [map_to_df(cm) for cm in all_commits]\n",
    "new_format_df = pd.DataFrame(new_format)\n",
    "print(new_format_df)\n",
    "new_format_df.to_csv('final_merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIFF_NS</th>\n",
       "      <th>DIFF_ND</th>\n",
       "      <th>DIFF_NF</th>\n",
       "      <th>DIFF_EN</th>\n",
       "      <th>SIZE_LA</th>\n",
       "      <th>SIZE_LD</th>\n",
       "      <th>SIZE_LT</th>\n",
       "      <th>SIZE_TFC</th>\n",
       "      <th>PURP_FIX</th>\n",
       "      <th>PURP_MR</th>\n",
       "      <th>...</th>\n",
       "      <th>HIST_NUC</th>\n",
       "      <th>EXP_EXP</th>\n",
       "      <th>EXP_REXP</th>\n",
       "      <th>EXP_SEXP</th>\n",
       "      <th>TEXT_CM</th>\n",
       "      <th>SKIP_DOC</th>\n",
       "      <th>SKIP_MET</th>\n",
       "      <th>SKIP_COM</th>\n",
       "      <th>SKIP_FRM</th>\n",
       "      <th>SKIP_BLD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LABEL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>...</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DIFF_NS  DIFF_ND  DIFF_NF  DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  \\\n",
       "LABEL                                                                  \n",
       "0         1631     1631     1631     1631     1631     1631     1631   \n",
       "1          321      321      321      321      321      321      321   \n",
       "\n",
       "       SIZE_TFC  PURP_FIX  PURP_MR  ...  HIST_NUC  EXP_EXP  EXP_REXP  \\\n",
       "LABEL                               ...                                \n",
       "0          1631      1631     1631  ...      1631     1631      1631   \n",
       "1           321       321      321  ...       321      321       321   \n",
       "\n",
       "       EXP_SEXP  TEXT_CM  SKIP_DOC  SKIP_MET  SKIP_COM  SKIP_FRM  SKIP_BLD  \n",
       "LABEL                                                                       \n",
       "0          1631     1631      1631      1631      1631      1631      1631  \n",
       "1           321      321       321       321       321       321       321  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_format_df.groupby('LABEL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78c9dbaee0ce7e029abb1cb526db50aa604f62f5e3e7eb23ab1c0d9506d0b132"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
