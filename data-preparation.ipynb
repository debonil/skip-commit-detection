{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, os,typing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name,data):\n",
    "    with open(f'data/{file_name}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = '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'\n",
    "headers = {'Accept': 'application/vnd.github+json',\n",
    "           'Authorization': 'Bearer {0}'.format(api_token)}\n",
    "def mine_repository(repo_url:str):\n",
    "    file_name = repo_url.replace('/','_')\n",
    "    if os.path.exists(f'data/{file_name}.json'):\n",
    "        print(f'Repository {repo_url} already expolred!') \n",
    "        return\n",
    "    print(f'Exploring repository {repo_url}:') \n",
    "    github_log_api_endpoint = f'https://api.github.com/repos/{repo_url}/commits'\n",
    "    r = requests.get(github_log_api_endpoint,headers=headers)\n",
    "    r_git_log = json.loads(r.content)\n",
    "    r.close()\n",
    "    if not isinstance(r_git_log, dict):\n",
    "        page_count = 1\n",
    "        links_str = r.headers['Link']\n",
    "        while links_str:\n",
    "            page_count += 1\n",
    "            next_link = [l.split(';')[0] for l in links_str.split(',') if '; rel=\"next\"' in l]\n",
    "            if next_link:\n",
    "                print(f'\\r Calling next page request = {next_link[0][1:-1]}',end='')\n",
    "                r = requests.get(next_link[0][1:-1],headers=headers)\n",
    "                r_git_log_next = json.loads(r.content)\n",
    "                if not isinstance(r_git_log_next, dict):\n",
    "                    r_git_log.extend(r_git_log_next)\n",
    "                    links_str = r.headers['Link']\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        print('\\r',end='')\n",
    "        print(f'\\t{len(r_git_log)} commits found in repository {repo_url} in {page_count} pages. {\" \"*30}')\n",
    "        for i,commit in enumerate(r_git_log):\n",
    "            print(f'\\r\\tDeep loading commit {i+1} of {len(r_git_log)}',end='')\n",
    "            commit_sha = commit[\"sha\"]\n",
    "            github_commit_api_endpoint = f'{github_log_api_endpoint}/{commit_sha}'\n",
    "            r = requests.get(github_commit_api_endpoint,headers=headers)\n",
    "            r_git_commit = json.loads(r.content)\n",
    "            r.close()\n",
    "            commit.update(r_git_commit) \n",
    "        print(f'\\r\\t{len(r_git_log)} commits deep loaded for repository {repo_url}.') \n",
    "        write_to_file(file_name,r_git_log)\n",
    "    else:\n",
    "        print(f'Could not access repository {repo_url}')\n",
    "        print(f'Error: {r_git_log}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository eBay/parallec already expolred!\n"
     ]
    }
   ],
   "source": [
    "target_repos_list = ['eBay/parallec','zixpo/candybar','mtsar/mtsar','GrammarViz2/grammarviz2_src']\n",
    "#target_repos_list = ['eBay/parallec']\n",
    "for repo_url in target_repos_list:\n",
    "    mine_repository(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 commits loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "repo_files = os.listdir('data/')\n",
    "\n",
    "all_commits:list[dict] = []\n",
    "for repo_file in repo_files:\n",
    "    with open('data/' + repo_file, 'r', encoding='utf-8') as f:\n",
    "        all_commits.extend(json.load(f)) \n",
    "\n",
    "print(f'{len(all_commits)} commits loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha': <class 'str'>, 'node_id': <class 'str'>, 'commit': <class 'dict'>, 'url': <class 'str'>, 'html_url': <class 'str'>, 'comments_url': <class 'str'>, 'author': <class 'NoneType'>, 'committer': <class 'NoneType'>, 'parents': <class 'list'>, 'stats': <class 'dict'>, 'files': <class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in all_commits[100].keys():\n",
    "    schema[k] = type(all_commits[100][k])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GitCommitBasic = typing.TypedDict('GitCommitBasic',\n",
    "                                  {'author': dict,\n",
    "                                   'comment_count': int,\n",
    "                                   'committer': dict,\n",
    "                                   'message': str,\n",
    "                                   'tree': dict,\n",
    "                                   'url': str,\n",
    "                                   'verification': dict})\n",
    "GitCommit = typing.TypedDict('GitCommit',\n",
    "                             {'sha': str,\n",
    "                              'node_id': str,\n",
    "                              'commit': GitCommitBasic,\n",
    "                              'url': str,\n",
    "                              'html_url': str,\n",
    "                              'comments_url': str,\n",
    "                              'author': dict,\n",
    "                              'committer': dict,\n",
    "                              'parents': list,\n",
    "                              'stats': dict,\n",
    "                              'files': list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'email', 'name']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m100\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m100\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mkeys()))\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m100\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommitter\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m120\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommitter\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print(sorted(all_commits[100]['commit']['author'].keys()))\n",
    "print(sorted(all_commits[100]['author'].keys()))\n",
    "\n",
    "print(sorted(all_commits[100]['commit']['committer'].keys()))\n",
    "print(sorted(all_commits[120]['committer'].keys()))\n",
    "\n",
    "print(all_commits[100]['commit']['url'])\n",
    "print(all_commits[100]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t'author': dict,\n",
      "\t'comment_count': int,\n",
      "\t'committer': dict,\n",
      "\t'message': str,\n",
      "\t'tree': dict,\n",
      "\t'url': str,\n",
      "\t'verification': dict,\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in sorted(all_commits[100]['commit'].keys()):\n",
    "    schema[k] = (type(all_commits[100]['commit'][k]).__qualname__)\n",
    "for k in schema:\n",
    "    print(f\"\\t'{k}': {schema[k]},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_defec_words =  [ 'bug', 'fix', 'fixed', 'defect', 'issue']\n",
    "\n",
    "list_merge_words =  [ 'Merge','merge','merged']\n",
    "\n",
    "list_format= ['formated','format','formatting']\n",
    "\n",
    "list_comment= ['commented','comment','commenting','comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(val_list:list[int]):\n",
    "    s = sum(val_list)\n",
    "    if s == 0:\n",
    "            return 0;\n",
    "    acc = 0\n",
    "    for val in val_list:       \n",
    "        acc += (val/s) ** 2\n",
    "    return 1- acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_changes(allFiles):\n",
    "    this_l = []\n",
    "    for file in allFiles:\n",
    "        this_l.append(file['changes'])\n",
    "    return calc_entropy(this_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def find_number_mod_directory(allFiles):\n",
    "    list_dir = []\n",
    "    for file in allFiles:\n",
    "            this_file = file['filename']\n",
    "            if \"/\" in this_file:\n",
    "                path = pathlib.PurePath(this_file)\n",
    "                list_dir.append(path.parent.name)\n",
    "    return len(set(list_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' \n",
    "files = []\n",
    "for cm in all_commits:\n",
    "    files.append(cm['files'])\n",
    "file_name = []\n",
    "for file in files:\n",
    "    for this_file in file:\n",
    "        file_name.append(this_file['filename'])\n",
    "   \n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #this_file = file['filename']\n",
    "            file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_ext)\n",
    "    return (set(file_exts))\n",
    "'''\n",
    "src_file_ext = ['R', 'jks', 'Procfile', 'classpath', 'properties', 'gradle', 'yml', 'spec', 'avi', 'png', 'ai', 'ser', 'json', 'sh', 'gpx', 'ttf', 'kwgt', 'csv', 'style', 'dtd', 'xml', 'Rproj', 'pro', 'conf', 'Dockerfile', 'pdf', 'gz', 'jpg', 'class', 'travis', 'html', 'bat', 'gradlew', 'mustache', 'java', 'klck', 'ico', 'pl', 'userdata/keepthisfile', 'jar', 'css', 'prefs', 'project', 'bib']\n",
    "\n",
    "meta_files_ext = ['Procfile', 'classpath', 'properties', 'gradle', 'csv', 'gradlew', 'prefs', 'project','.gitignore']\n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 0 if any(word in file_ext_cm for word in src_file_ext) else 1\n",
    "\n",
    "def get_file_meta(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 1 if any(word in file_ext_cm for word in meta_files_ext) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "(146, 10)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\debon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef get_text_tfidf_thiscommit(message):\\n    return model.transform(message.split(\" \"))\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "all_messages = []\n",
    "def get_text_tfidf():\n",
    "  \n",
    "    for this_comit in all_commits:\n",
    "        all_messages.append(this_comit['commit']['message'])\n",
    "    documents = all_messages\n",
    "\n",
    "    # create set of stopwords to remove\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(english_stop_words)\n",
    "\n",
    "    # check if word in stop words\n",
    "    print('when' in stop_words)  # True\n",
    "    print('il' in stop_words)  # True\n",
    "\n",
    "    # else add word to the set\n",
    "    print('went' in stop_words)  # False\n",
    "    stop_words.add('skip')\n",
    "    stop_words.add(\"ci\")\n",
    "\n",
    "    # create tf-idf from original documents\n",
    "    tfidf = TfidfVectorizer(stop_words=stop_words,use_idf=True,max_features =10,)\n",
    "    x = tfidf.fit_transform(documents)\n",
    "    print(x.shape)\n",
    "    #df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "    #print(df_tfidf)\n",
    "    #print(x)\n",
    "    print(type(x))\n",
    "    tfidf\n",
    "\n",
    "    featuere_name = tfidf.get_feature_names()\n",
    "\n",
    "    for col in x.nonzero()[1]:\n",
    "        pass #print(featuere_name[col], \"-\", x[0,col])\n",
    "    #print({c: s[s > 0] for c, s in zip(df_tfidf, df_tfidf.T.values)})\n",
    "    return x,featuere_name\n",
    "\n",
    "x,featuere_name= get_text_tfidf()\n",
    "\n",
    "print()\n",
    "\n",
    "'''\n",
    "def get_text_tfidf_thiscommit(message):\n",
    "    return model.transform(message.split(\" \"))\n",
    "'''\n",
    "\n",
    "\n",
    "#print(get_text_tfidf_thiscommit(all_commits[100]['commit']['message']))\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_to_df(commit_dict:GitCommit):\n",
    "\n",
    "    new_dict = {}\n",
    "\n",
    "    new_dict['DIFF_NS'] =0\n",
    "    new_dict['DIFF_ND'] = find_number_mod_directory(commit_dict['files'])\n",
    "    new_dict['DIFF_NF'] = int(len(commit_dict['files']))\n",
    "    new_dict['DIFF_EN'] = get_entropy_changes(commit_dict['files'])\n",
    " \n",
    "    new_dict['SIZE_LA'] = int(commit_dict['stats']['additions'])\n",
    "    new_dict['SIZE_LD'] = int(commit_dict['stats']['deletions'])\n",
    "    new_dict['SIZE_LT'] = 0\n",
    "    new_dict['SIZE_TFC'] = 0\n",
    "\n",
    "    new_dict['PURP_FIX'] =  1 if any(word.lower() in commit_dict['commit']['message'] for word in list_defec_words) else 0\n",
    "    new_dict['PURP_MR'] = 1 if any(word.lower() in commit_dict['commit']['message'] for word in list_merge_words) else 0\n",
    "    new_dict['PURP_CFT'] =0\n",
    "\n",
    "    new_dict['HIST_NDEV'] =0\n",
    "    new_dict['HIST_AGE'] =0\n",
    "    new_dict['HIST_NUC'] =0\n",
    "\n",
    "    new_dict['EXP_EXP'] =0\n",
    "    new_dict['EXP_REXP'] =0\n",
    "    new_dict['EXP_SEXP'] =0\n",
    "\n",
    "    new_dict['TEXT_CM'] =0\n",
    "\n",
    "    new_dict['SKIP_DOC'] =get_file_types_value(commit_dict['files'])\n",
    "    new_dict['SKIP_MET'] =get_file_meta(commit_dict['files'])\n",
    "    new_dict['SKIP_COM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_comment) else 0\n",
    "    new_dict['SKIP_FRM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_format) else 0\n",
    "    new_dict['SKIP_BLD'] =0\n",
    "\n",
    "    new_dict['LABEL'] = int(('[ci skip]' in commit_dict['commit']['message'].lower()) or ('[skip ci]' in commit_dict['commit']['message'].lower()))\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DIFF_NS  DIFF_ND  DIFF_NF   DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  SIZE_TFC  \\\n",
      "0          0        1        1  0.000000        7        0        0         0   \n",
      "1          0        0        1  0.000000        3        0        0         0   \n",
      "2          0        0        1  0.000000        5        0        0         0   \n",
      "3          0        2        2  0.345679       24       12        0         0   \n",
      "4          0        0        1  0.000000        1        1        0         0   \n",
      "..       ...      ...      ...       ...      ...      ...      ...       ...   \n",
      "141        0       14       44  0.737418      178       63        0         0   \n",
      "142        0        1        6  0.696759        4       68        0         0   \n",
      "143        0       30      177  0.979887    25255        1        0         0   \n",
      "144        0        0        1  0.000000        1        0        0         0   \n",
      "145        0        0        1  0.000000      202        0        0         0   \n",
      "\n",
      "     PURP_FIX  PURP_MR  ...  EXP_EXP  EXP_REXP  EXP_SEXP  TEXT_CM  SKIP_DOC  \\\n",
      "0           0        0  ...        0         0         0        0         1   \n",
      "1           0        0  ...        0         0         0        0         1   \n",
      "2           0        0  ...        0         0         0        0         1   \n",
      "3           1        0  ...        0         0         0        0         1   \n",
      "4           0        0  ...        0         0         0        0         1   \n",
      "..        ...      ...  ...      ...       ...       ...      ...       ...   \n",
      "141         0        0  ...        0         0         0        0         1   \n",
      "142         0        0  ...        0         0         0        0         1   \n",
      "143         0        0  ...        0         0         0        0         1   \n",
      "144         0        0  ...        0         0         0        0         1   \n",
      "145         0        0  ...        0         0         0        0         1   \n",
      "\n",
      "     SKIP_MET  SKIP_COM  SKIP_FRM  SKIP_BLD  LABEL  \n",
      "0           0         0         0         0      0  \n",
      "1           0         0         0         0      0  \n",
      "2           0         0         0         0      0  \n",
      "3           0         0         0         0      0  \n",
      "4           0         0         0         0      0  \n",
      "..        ...       ...       ...       ...    ...  \n",
      "141         0         0         0         0      0  \n",
      "142         0         0         0         0      0  \n",
      "143         0         0         0         0      0  \n",
      "144         0         0         0         0      0  \n",
      "145         0         0         0         0      0  \n",
      "\n",
      "[146 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "new_format = [map_to_df(cm) for cm in all_commits]\n",
    "new_format_df = pd.DataFrame(new_format)\n",
    "print(new_format_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,fn in enumerate(featuere_name):\n",
    "    new_format_df['CM_'+fn]=x.A.T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add',\n",
       " 'cn',\n",
       " 'doc',\n",
       " 'fix',\n",
       " 'merge',\n",
       " 'pull',\n",
       " 'readme',\n",
       " 'request',\n",
       " 'test',\n",
       " 'update']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuere_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIFF_NS</th>\n",
       "      <th>DIFF_ND</th>\n",
       "      <th>DIFF_NF</th>\n",
       "      <th>DIFF_EN</th>\n",
       "      <th>SIZE_LA</th>\n",
       "      <th>SIZE_LD</th>\n",
       "      <th>SIZE_LT</th>\n",
       "      <th>SIZE_TFC</th>\n",
       "      <th>PURP_FIX</th>\n",
       "      <th>PURP_MR</th>\n",
       "      <th>...</th>\n",
       "      <th>CM_add</th>\n",
       "      <th>CM_cn</th>\n",
       "      <th>CM_doc</th>\n",
       "      <th>CM_fix</th>\n",
       "      <th>CM_merge</th>\n",
       "      <th>CM_pull</th>\n",
       "      <th>CM_readme</th>\n",
       "      <th>CM_request</th>\n",
       "      <th>CM_test</th>\n",
       "      <th>CM_update</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LABEL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DIFF_NS  DIFF_ND  DIFF_NF  DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  \\\n",
       "LABEL                                                                  \n",
       "0           73       73       73       73       73       73       73   \n",
       "1           73       73       73       73       73       73       73   \n",
       "\n",
       "       SIZE_TFC  PURP_FIX  PURP_MR  ...  CM_add  CM_cn  CM_doc  CM_fix  \\\n",
       "LABEL                               ...                                  \n",
       "0            73        73       73  ...      73     73      73      73   \n",
       "1            73        73       73  ...      73     73      73      73   \n",
       "\n",
       "       CM_merge  CM_pull  CM_readme  CM_request  CM_test  CM_update  \n",
       "LABEL                                                                \n",
       "0            73       73         73          73       73         73  \n",
       "1            73       73         73          73       73         73  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_format_df.groupby('LABEL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_format_df.to_csv('final_merged',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78c9dbaee0ce7e029abb1cb526db50aa604f62f5e3e7eb23ab1c0d9506d0b132"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
