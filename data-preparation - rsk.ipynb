{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, os,typing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file_name,data):\n",
    "    with open(f'data/{file_name}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = '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'\n",
    "headers = {'Accept': 'application/vnd.github+json',\n",
    "           'Authorization': 'Bearer {0}'.format(api_token)}\n",
    "def mine_repository(repo_url:str):\n",
    "    file_name = repo_url.replace('/','_')\n",
    "    if os.path.exists(f'data/{file_name}.json'):\n",
    "        print(f'Repository {repo_url} already expolred!') \n",
    "        return\n",
    "    print(f'Exploring repository {repo_url}:') \n",
    "    github_log_api_endpoint = f'https://api.github.com/repos/{repo_url}/commits'\n",
    "    r = requests.get(github_log_api_endpoint,headers=headers)\n",
    "    r_git_log = json.loads(r.content)\n",
    "    r.close()\n",
    "    if not isinstance(r_git_log, dict):\n",
    "        page_count = 1\n",
    "        links_str = r.headers['Link']\n",
    "        while links_str:\n",
    "            page_count += 1\n",
    "            next_link = [l.split(';')[0] for l in links_str.split(',') if '; rel=\"next\"' in l]\n",
    "            if next_link:\n",
    "                print(f'\\r Calling next page request = {next_link[0][1:-1]}',end='')\n",
    "                r = requests.get(next_link[0][1:-1],headers=headers)\n",
    "                r_git_log_next = json.loads(r.content)\n",
    "                if not isinstance(r_git_log_next, dict):\n",
    "                    r_git_log.extend(r_git_log_next)\n",
    "                    links_str = r.headers['Link']\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        print('\\r',end='')\n",
    "        print(f'\\t{len(r_git_log)} commits found in repository {repo_url} in {page_count} pages. {\" \"*30}')\n",
    "        for i,commit in enumerate(r_git_log):\n",
    "            print(f'\\r\\tDeep loading commit {i+1} of {len(r_git_log)}',end='')\n",
    "            commit_sha = commit[\"sha\"]\n",
    "            github_commit_api_endpoint = f'{github_log_api_endpoint}/{commit_sha}'\n",
    "            r = requests.get(github_commit_api_endpoint,headers=headers)\n",
    "            r_git_commit = json.loads(r.content)\n",
    "            r.close()\n",
    "            commit.update(r_git_commit) \n",
    "        print(f'\\r\\t{len(r_git_log)} commits deep loaded for repository {repo_url}.') \n",
    "        write_to_file(file_name,r_git_log)\n",
    "    else:\n",
    "        print(f'Could not access repository {repo_url}')\n",
    "        print(f'Error: {r_git_log}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository eBay/parallec already expolred!\n",
      "Repository zixpo/candybar already expolred!\n",
      "Repository mtsar/mtsar already expolred!\n",
      "Repository GrammarViz2/grammarviz2_src already expolred!\n"
     ]
    }
   ],
   "source": [
    "target_repos_list = ['eBay/parallec','zixpo/candybar','mtsar/mtsar','GrammarViz2/grammarviz2_src']\n",
    "#target_repos_list = ['eBay/parallec']\n",
    "for repo_url in target_repos_list:\n",
    "    mine_repository(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6417 commits loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "repo_files = os.listdir('data/')\n",
    "\n",
    "all_commits:list[dict] = []\n",
    "for repo_file in repo_files:\n",
    "    with open('data/' + repo_file, 'r', encoding='utf-8') as f:\n",
    "        all_commits.extend(json.load(f)) \n",
    "\n",
    "print(f'{len(all_commits)} commits loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha': <class 'str'>, 'node_id': <class 'str'>, 'commit': <class 'dict'>, 'url': <class 'str'>, 'html_url': <class 'str'>, 'comments_url': <class 'str'>, 'author': <class 'NoneType'>, 'committer': <class 'NoneType'>, 'parents': <class 'list'>, 'stats': <class 'dict'>, 'files': <class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in all_commits[100].keys():\n",
    "    schema[k] = type(all_commits[100][k])\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GitCommitBasic = typing.TypedDict('GitCommitBasic',\n",
    "                                  {'author': dict,\n",
    "                                   'comment_count': int,\n",
    "                                   'committer': dict,\n",
    "                                   'message': str,\n",
    "                                   'tree': dict,\n",
    "                                   'url': str,\n",
    "                                   'verification': dict})\n",
    "GitCommit = typing.TypedDict('GitCommit',\n",
    "                             {'sha': str,\n",
    "                              'node_id': str,\n",
    "                              'commit': GitCommitBasic,\n",
    "                              'url': str,\n",
    "                              'html_url': str,\n",
    "                              'comments_url': str,\n",
    "                              'author': dict,\n",
    "                              'committer': dict,\n",
    "                              'parents': list,\n",
    "                              'stats': dict,\n",
    "                              'files': list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'email', 'name']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ravi_Kumar2\\PycharmProjects\\skip-commit-detection\\data-preparation - rsk.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ravi_Kumar2/PycharmProjects/skip-commit-detection/data-preparation%20-%20rsk.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m1000\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ravi_Kumar2/PycharmProjects/skip-commit-detection/data-preparation%20-%20rsk.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m1000\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mkeys()))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ravi_Kumar2/PycharmProjects/skip-commit-detection/data-preparation%20-%20rsk.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m1000\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommitter\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ravi_Kumar2/PycharmProjects/skip-commit-detection/data-preparation%20-%20rsk.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msorted\u001b[39m(all_commits[\u001b[39m1000\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcommitter\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print(sorted(all_commits[1000]['commit']['author'].keys()))\n",
    "print(sorted(all_commits[1000]['author'].keys()))\n",
    "\n",
    "print(sorted(all_commits[1000]['commit']['committer'].keys()))\n",
    "print(sorted(all_commits[1000]['committer'].keys()))\n",
    "\n",
    "print(all_commits[100]['commit']['url'])\n",
    "print(all_commits[100]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t'author': dict,\n",
      "\t'comment_count': int,\n",
      "\t'committer': dict,\n",
      "\t'message': str,\n",
      "\t'tree': dict,\n",
      "\t'url': str,\n",
      "\t'verification': dict,\n"
     ]
    }
   ],
   "source": [
    "schema = {}\n",
    "for k in sorted(all_commits[100]['commit'].keys()):\n",
    "    schema[k] = (type(all_commits[100]['commit'][k]).__qualname__)\n",
    "for k in schema:\n",
    "    print(f\"\\t'{k}': {schema[k]},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_defec_words =  [ 'bug', 'fix', 'fixed', 'defect', 'issue']\n",
    "\n",
    "list_merge_words =  [ 'Merge','merge','merged']\n",
    "\n",
    "list_format= ['formated','format','formatting']\n",
    "\n",
    "list_comment= ['commented','comment','commenting','comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(val_list:list[int]):\n",
    "    s = sum(val_list)\n",
    "    if s == 0:\n",
    "            return 0;\n",
    "    acc = 0\n",
    "    for val in val_list:       \n",
    "        acc += (val/s) ** 2\n",
    "    return 1- acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_changes(allFiles):\n",
    "    this_l = []\n",
    "    for file in allFiles:\n",
    "        this_l.append(file['changes'])\n",
    "    return calc_entropy(this_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "def find_number_mod_directory(allFiles):\n",
    "    list_dir = []\n",
    "    for file in allFiles:\n",
    "            this_file = file['filename']\n",
    "            if \"/\" in this_file:\n",
    "                path = pathlib.PurePath(this_file)\n",
    "                list_dir.append(path.parent.name)\n",
    "    return len(set(list_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''' \n",
    "files = []\n",
    "for cm in all_commits:\n",
    "    files.append(cm['files'])\n",
    "file_name = []\n",
    "for file in files:\n",
    "    for this_file in file:\n",
    "        file_name.append(this_file['filename'])\n",
    "   \n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #this_file = file['filename']\n",
    "            file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_ext)\n",
    "    return (set(file_exts))\n",
    "'''\n",
    "src_file_ext = ['R', 'jks', 'Procfile', 'classpath', 'properties', 'gradle', 'yml', 'spec', 'avi', 'png', 'ai', 'ser', 'json', 'sh', 'gpx', 'ttf', 'kwgt', 'csv', 'style', 'dtd', 'xml', 'Rproj', 'pro', 'conf', 'Dockerfile', 'pdf', 'gz', 'jpg', 'class', 'travis', 'html', 'bat', 'gradlew', 'mustache', 'java', 'klck', 'ico', 'pl', 'userdata/keepthisfile', 'jar', 'css', 'prefs', 'project', 'bib']\n",
    "\n",
    "meta_files_ext = ['Procfile', 'classpath', 'properties', 'gradle', 'csv', 'gradlew', 'prefs', 'project','.gitignore']\n",
    "\n",
    "def get_file_types_value(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 0 if any(word in file_ext_cm for word in src_file_ext) else 1\n",
    "\n",
    "def get_file_meta(allFiles):\n",
    "    file_exts = []\n",
    "    for file in allFiles:\n",
    "            #print(file)\n",
    "            this_file = file['filename']\n",
    "            #print(this_file)\n",
    "            file_extension = pathlib.Path(this_file).suffix\n",
    "            #file_ext = file.split(\".\")[-1]\n",
    "            file_exts.append(file_extension)\n",
    "    file_ext_cm = list(set(file_exts))\n",
    "    return 1 if any(word in file_ext_cm for word in meta_files_ext) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "(6417, 4654)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ravi_Kumar2\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['skip'] not in stop_words.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ravi_Kumar2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef get_text_tfidf_thiscommit(message):\\n    return model.transform(message.split(\" \"))\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "all_messages = []\n",
    "def get_text_tfidf():\n",
    "  \n",
    "    for this_comit in all_commits:\n",
    "        all_messages.append(this_comit['commit']['message'])\n",
    "    documents = all_messages\n",
    "\n",
    "    # create set of stopwords to remove\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(english_stop_words)\n",
    "\n",
    "    # check if word in stop words\n",
    "    print('when' in stop_words)  # True\n",
    "    print('il' in stop_words)  # True\n",
    "\n",
    "    # else add word to the set\n",
    "    print('went' in stop_words)  # False\n",
    "    stop_words.add('[skip ci]')\n",
    "    stop_words.add(\"[ci skip]\")\n",
    "\n",
    "    # create tf-idf from original documents\n",
    "    tfidf = TfidfVectorizer(stop_words=stop_words,use_idf=True)\n",
    "    x = tfidf.fit_transform(documents)\n",
    "    print(x.shape)\n",
    "    #df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "    #print(df_tfidf)\n",
    "    #print(x)\n",
    "    print(type(x))\n",
    "    tfidf\n",
    "\n",
    "    featuere_name = tfidf.get_feature_names()\n",
    "\n",
    "    for col in x.nonzero()[1]:\n",
    "        pass #print(featuere_name[col], \"-\", x[0,col])\n",
    "    #print({c: s[s > 0] for c, s in zip(df_tfidf, df_tfidf.T.values)})\n",
    "    return x\n",
    "\n",
    "x= get_text_tfidf()\n",
    "\n",
    "print()\n",
    "\n",
    "'''\n",
    "def get_text_tfidf_thiscommit(message):\n",
    "    return model.transform(message.split(\" \"))\n",
    "'''\n",
    "\n",
    "\n",
    "#print(get_text_tfidf_thiscommit(all_commits[100]['commit']['message']))\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "def get_no_developer_for_file():\n",
    "    file_dev_stat = []\n",
    "    file_set = set()\n",
    "    developer_set= set()\n",
    "    file_stasts = {}\n",
    "    for this_commit in all_commits:\n",
    "        all_files_this_commit = this_commit['files']\n",
    "        for this_file in all_files_this_commit:\n",
    "                file_dev_stat.append({this_file['filename'] :this_commit['commit']['author']['name'] })\n",
    "                file_set.add(this_file['filename'])\n",
    "                developer_set.add(this_commit['commit']['author']['name'])\n",
    "    print(len(developer_set))\n",
    "    for this_file_set in file_set:\n",
    "        devlopers = set()\n",
    "        for combi in file_dev_stat:\n",
    "            file_name = list(combi.keys())[0]\n",
    "            if str(file_name) in str(this_file_set):\n",
    "                devlopers.add(list(combi.values())[0])\n",
    "        file_stasts[this_file_set] = len(devlopers) \n",
    "    #print(file_stasts)      \n",
    "    return file_stasts\n",
    "                \n",
    "file_stats = get_no_developer_for_file()\n",
    "#print(file_stats)\n",
    "\n",
    "def get_no_dev_change_files(all_files_commits):\n",
    "    files_stats_count = []\n",
    "    for this_file in all_files_commits:\n",
    "        #print(file_stats[this_file['filename']])\n",
    "        if this_file['filename'] not in file_stats:\n",
    "            print(this_file['filename'] )\n",
    "        else:\n",
    "            files_stats_count.append(int(file_stats[str(this_file['filename'])]))\n",
    "        return max(files_stats_count)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_to_df(commit_dict:GitCommit):\n",
    "\n",
    "    new_dict = {}\n",
    "\n",
    "    new_dict['DIFF_NS'] =0\n",
    "    new_dict['DIFF_ND'] = find_number_mod_directory(commit_dict['files'])\n",
    "    new_dict['DIFF_NF'] = int(len(commit_dict['files']))\n",
    "    new_dict['DIFF_EN'] = get_entropy_changes(commit_dict['files'])\n",
    " \n",
    "    new_dict['SIZE_LA'] = int(commit_dict['stats']['additions'])\n",
    "    new_dict['SIZE_LD'] = int(commit_dict['stats']['deletions'])\n",
    "    new_dict['SIZE_LT'] = 0\n",
    "    new_dict['SIZE_TFC'] = 0\n",
    "\n",
    "    new_dict['PURP_FIX'] =  1 if any(word.lower() in commit_dict['commit']['message'] for word in list_defec_words) else 0\n",
    "    new_dict['PURP_MR'] = 1 if any(word.lower() in commit_dict['commit']['message'] for word in list_merge_words) else 0\n",
    "    new_dict['PURP_CFT'] =0\n",
    "\n",
    "    new_dict['HIST_NDEV'] =get_no_dev_change_files(commit_dict['files'])\n",
    "    new_dict['HIST_AGE'] =0\n",
    "    new_dict['HIST_NUC'] =0\n",
    "\n",
    "    new_dict['EXP_EXP'] =0\n",
    "    new_dict['EXP_REXP'] =0\n",
    "    new_dict['EXP_SEXP'] =0\n",
    "\n",
    "    new_dict['TEXT_CM'] =0\n",
    "\n",
    "    new_dict['SKIP_DOC'] =get_file_types_value(commit_dict['files'])\n",
    "    new_dict['SKIP_MET'] =get_file_meta(commit_dict['files'])\n",
    "    new_dict['SKIP_COM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_comment) else 0\n",
    "    new_dict['SKIP_FRM'] =1 if any(word.lower() in commit_dict['commit']['message'] for word in list_format) else 0\n",
    "    new_dict['SKIP_BLD'] =0\n",
    "\n",
    "    new_dict['LABEL'] = int(('[ci skip]' in commit_dict['commit']['message'].lower()) or ('[skip ci]' in commit_dict['commit']['message'].lower()))\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DIFF_NS  DIFF_ND  DIFF_NF   DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  \\\n",
      "0           0        1        1  0.000000        7        0        0   \n",
      "1           0        0        1  0.000000        3        0        0   \n",
      "2           0        0        1  0.000000        5        0        0   \n",
      "3           0        2        2  0.345679       24       12        0   \n",
      "4           0        0        1  0.000000        1        1        0   \n",
      "...       ...      ...      ...       ...      ...      ...      ...   \n",
      "6412        0        8       16  0.778574      266       54        0   \n",
      "6413        0       34      158  0.974474      735      724        0   \n",
      "6414        0        7        9  0.838786       22       19        0   \n",
      "6415        0        3        3  0.431602       74       75        0   \n",
      "6416        0       57      300  0.977624    49086        0        0   \n",
      "\n",
      "      SIZE_TFC  PURP_FIX  PURP_MR  ...  EXP_EXP  EXP_REXP  EXP_SEXP  TEXT_CM  \\\n",
      "0            0         0        0  ...        0         0         0        0   \n",
      "1            0         0        0  ...        0         0         0        0   \n",
      "2            0         0        0  ...        0         0         0        0   \n",
      "3            0         1        0  ...        0         0         0        0   \n",
      "4            0         0        0  ...        0         0         0        0   \n",
      "...        ...       ...      ...  ...      ...       ...       ...      ...   \n",
      "6412         0         0        0  ...        0         0         0        0   \n",
      "6413         0         0        0  ...        0         0         0        0   \n",
      "6414         0         0        0  ...        0         0         0        0   \n",
      "6415         0         0        0  ...        0         0         0        0   \n",
      "6416         0         0        0  ...        0         0         0        0   \n",
      "\n",
      "      SKIP_DOC  SKIP_MET  SKIP_COM  SKIP_FRM  SKIP_BLD  LABEL  \n",
      "0            1         0         0         0         0      0  \n",
      "1            1         0         0         0         0      0  \n",
      "2            1         0         0         0         0      0  \n",
      "3            1         0         0         0         0      0  \n",
      "4            1         0         0         0         0      0  \n",
      "...        ...       ...       ...       ...       ...    ...  \n",
      "6412         1         0         0         0         0      0  \n",
      "6413         1         0         0         0         0      0  \n",
      "6414         1         0         0         0         0      0  \n",
      "6415         1         0         0         0         0      0  \n",
      "6416         1         0         0         0         0      0  \n",
      "\n",
      "[6417 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "new_format = [map_to_df(cm) for cm in all_commits]\n",
    "#tf_def = x[0]\n",
    "#df = pd.DataFrame(tf_def.T.todense(), columns=[\"tfidf\"])\n",
    "#print(df)\n",
    "\n",
    "#new_format[\"TEXT_CM\"] = df[\"tfidf\"]\n",
    "new_format_df = pd.DataFrame(new_format)\n",
    "print(new_format_df)\n",
    "new_format_df.to_csv('final_merged',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIFF_NS</th>\n",
       "      <th>DIFF_ND</th>\n",
       "      <th>DIFF_NF</th>\n",
       "      <th>DIFF_EN</th>\n",
       "      <th>SIZE_LA</th>\n",
       "      <th>SIZE_LD</th>\n",
       "      <th>SIZE_LT</th>\n",
       "      <th>SIZE_TFC</th>\n",
       "      <th>PURP_FIX</th>\n",
       "      <th>PURP_MR</th>\n",
       "      <th>...</th>\n",
       "      <th>HIST_NUC</th>\n",
       "      <th>EXP_EXP</th>\n",
       "      <th>EXP_REXP</th>\n",
       "      <th>EXP_SEXP</th>\n",
       "      <th>TEXT_CM</th>\n",
       "      <th>SKIP_DOC</th>\n",
       "      <th>SKIP_MET</th>\n",
       "      <th>SKIP_COM</th>\n",
       "      <th>SKIP_FRM</th>\n",
       "      <th>SKIP_BLD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LABEL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>...</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DIFF_NS  DIFF_ND  DIFF_NF  DIFF_EN  SIZE_LA  SIZE_LD  SIZE_LT  \\\n",
       "LABEL                                                                  \n",
       "0         1631     1631     1631     1631     1631     1631     1631   \n",
       "1          321      321      321      321      321      321      321   \n",
       "\n",
       "       SIZE_TFC  PURP_FIX  PURP_MR  ...  HIST_NUC  EXP_EXP  EXP_REXP  \\\n",
       "LABEL                               ...                                \n",
       "0          1631      1631     1631  ...      1631     1631      1631   \n",
       "1           321       321      321  ...       321      321       321   \n",
       "\n",
       "       EXP_SEXP  TEXT_CM  SKIP_DOC  SKIP_MET  SKIP_COM  SKIP_FRM  SKIP_BLD  \n",
       "LABEL                                                                       \n",
       "0          1631     1631      1631      1631      1631      1631      1631  \n",
       "1           321      321       321       321       321       321       321  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_format_df.groupby('LABEL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78c9dbaee0ce7e029abb1cb526db50aa604f62f5e3e7eb23ab1c0d9506d0b132"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
